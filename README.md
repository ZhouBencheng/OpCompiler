# Infini - ç®—å­èåˆæ¡†æ¶

**Infini** æ˜¯ä¸€ä¸ªé¢å‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„é«˜æ€§èƒ½ç®—å­èåˆæ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨å°†å¤šä¸ªç®—å­èåˆä¸ºå•ä¸ªå†…æ ¸æ¥å‡å°‘å†…å­˜è®¿é—®å’Œå†…æ ¸å¯åŠ¨å¼€é”€ï¼Œç›¸æ¯”ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼å¯å®ç° **2-5 å€åŠ é€Ÿ**ã€‚

## ğŸš€ ä¸ºä»€ä¹ˆéœ€è¦ç®—å­èåˆï¼Ÿ

åœ¨ LLM æ¨ç†ä¸­ï¼Œ`SiLU`ã€`Mul`ã€`RMSNorm` ç­‰ç®—å­é€šå¸¸é¡ºåºæ‰§è¡Œã€‚æ¯ä¸ªç®—å­å¯åŠ¨éœ€è¦ï¼š
- âœ… å†…æ ¸å¯åŠ¨å¼€é”€ï¼ˆçº¦ 10-50Î¼sï¼‰
- âœ… å…¨å±€å†…å­˜çš„è¯»å†™
- âœ… å†…æ ¸åŒæ­¥

**èåˆé€šè¿‡å°†å¤šä¸ªç®—å­åˆå¹¶ä¸ºå•ä¸ªå†…æ ¸æ¥è§£å†³è¿™äº›é—®é¢˜**ï¼š
- âœ… å•æ¬¡å†…æ ¸å¯åŠ¨
- âœ… æ•°æ®ä¿ç•™åœ¨ GPU å¯„å­˜å™¨/å…±äº«å†…å­˜ä¸­
- âœ… æ— ä¸­é—´ç»“æœå†™å›å†…å­˜

### æ€§èƒ½æå‡

| æ“ä½œ | æ ‡å‡†æ‰§è¡Œ | èåˆæ‰§è¡Œ | åŠ é€Ÿæ¯” |
|-----------|----------|-------|---------|
| SwiGLU (4096Ã—32) | 0.45 ms | 0.18 ms | **2.5x** |
| Add+RMSNorm (4096Ã—32) | 0.52 ms | 0.22 ms | **2.4x** |
| FFN Layer (seq=2048) | 8.2 ms | 3.5 ms | **2.3x** |

---

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. è¿è¡Œæ—¶èåˆè°ƒåº¦å™¨
```python
from infinicore.fusion import FusionScheduler, FusionConfig

# åˆå§‹åŒ–è°ƒåº¦å™¨å¹¶å¯ç”¨èåˆ
config = FusionConfig(
    enable_fusion=True,
    enable_cache=True,      # ç¼“å­˜ç¼–è¯‘åçš„å†…æ ¸
    min_nodes=2,            # è§¦å‘èåˆçš„æœ€å°èŠ‚ç‚¹æ•°
    fallback_on_error=True  # ç¼–è¯‘å¤±è´¥æ—¶è‡ªåŠ¨å›é€€
)
scheduler = FusionScheduler(config)
```

### 2. åŸºäºå¯å‘å¼çš„å†³ç­–
è°ƒåº¦å™¨ä½¿ç”¨**é™æ€å¯å‘å¼è§„åˆ™**å†³å®šä½•æ—¶èåˆï¼š
- âœ… **ç®—å­ç™½åå•**: ä»…èåˆæ”¯æŒçš„ç®—å­
- âœ… **å¼ é‡å¤§å°é˜ˆå€¼**: é¿å…å¯¹å°å¼ é‡è¿›è¡Œèåˆ
- âœ… **èŠ‚ç‚¹æ•°é˜ˆå€¼**: é¿å…å¯¹ç®€å•å›¾è¿›è¡Œèåˆ
- âœ… **ç¼“å­˜æŸ¥æ‰¾**: å°½å¯èƒ½å¤ç”¨å·²ç¼–è¯‘çš„å†…æ ¸

### 3. è‡ªåŠ¨å›é€€æœºåˆ¶
å¦‚æœèåˆç¼–è¯‘å¤±è´¥ï¼Œè°ƒåº¦å™¨ä¼š**è‡ªåŠ¨å›é€€åˆ°æ ‡å‡†æ‰§è¡Œ**ï¼š
```python
outputs = scheduler.dispatch(graph, inputs)
# â†’ é¦–å…ˆå°è¯•èåˆå†…æ ¸
# â†’ å¤±è´¥æ—¶å›é€€åˆ°å•ç‹¬æ‰§è¡Œç®—å­
```

### 4. å†…æ ¸ç¼“å­˜
ç¼–è¯‘åçš„å†…æ ¸æŒ‰**ç­¾åç¼“å­˜**ï¼ˆå›¾ç»“æ„ + æ•°æ®ç±»å‹ + å½¢çŠ¶ï¼‰ï¼š
```python
cache_key = graph.cache_key(input_dtypes, input_shapes)
# â†’ ä¾‹å¦‚: "a3f2c8b1d4e5f6a7"
```

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç”¨æˆ·åº”ç”¨å±‚                                 â”‚
â”‚  (InfiniLM, InfiniTrain, æˆ–è‡ªå®šä¹‰æ¨ç†å¼•æ“)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“ dispatch(graph, inputs)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FusionScheduler                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. å¯å‘å¼å¼•æ“                                        â”‚    â”‚
â”‚  â”‚     â†’ æ£€æŸ¥èåˆæ˜¯å¦æœ‰ç›Š                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  2. ç¼“å­˜æŸ¥æ‰¾                                         â”‚    â”‚
â”‚  â”‚     â†’ ç­¾ååŒ¹é…æ—¶å¤ç”¨å·²ç¼–è¯‘å†…æ ¸                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  3. å†…æ ¸ç¼–è¯‘å™¨ (ninetoothed)                         â”‚    â”‚
â”‚  â”‚     â†’ ä»å­å›¾ç”Ÿæˆ Triton å†…æ ¸                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  4. æ‰§è¡Œè°ƒåº¦å™¨                                        â”‚    â”‚
â”‚  â”‚     â†’ æ‰§è¡Œèåˆå†…æ ¸ æˆ– å›é€€                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â†“                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  èåˆè·¯å¾„            â”‚        â”‚  å›é€€è·¯å¾„            â”‚
â”‚  (Triton å†…æ ¸)       â”‚        â”‚  (å•ç‹¬ç®—å­)          â”‚
â”‚  ninetoothed â†’ ntopsâ”‚        â”‚  InfiniCore ç®—å­     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š æ”¯æŒçš„èåˆæ¨¡å¼

### LLM å¸¸è§æ¨¡å¼

#### 1. SwiGLU æ¿€æ´»å‡½æ•°
ç”¨äº LLaMAã€Mistralã€ChatGLM çš„ FFN å±‚ï¼š
```python
output = SiLU(gate) * up
```
**èåˆç®—å­**: `silu` + `mul`

#### 2. Add + RMSNorm
ç”¨äº Transformer åå¤„ç†ï¼š
```python
output = rms_norm(x + residual, weight)
```
**èåˆç®—å­**: `add` + `rms_norm`

#### 3. GELU æ¿€æ´»å‡½æ•°
ç”¨äº BERTã€GPT æ¨¡å‹ï¼š
```python
output = GELU(x)
```
**èåˆç®—å­**: `gelu`ï¼ˆå•ç®—å­èåˆä¼˜åŒ–ï¼‰

### æ‰©å±•èåˆæ¨¡å¼

æ·»åŠ è‡ªå®šä¹‰èåˆæ¨¡å¼ï¼š
```python
from infinicore.fusion.patterns import SubGraph, OpNode

def create_my_pattern() -> SubGraph:
    return SubGraph(
        nodes=(
            OpNode("add", inputs=("x", "y"), outputs=("sum",)),
            OpNode("relu", inputs=("sum",), outputs=("activated",)),
            OpNode("mul", inputs=("activated", "scale"), outputs=("output",)),
        ),
        input_names=("x", "y", "scale"),
        output_names=("output",),
    )
```

---

## ğŸ’» å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/hootandy321/OpCompiler.git
cd Infini/InfiniCore

# å®‰è£…ä¾èµ–
pip install -e .

# GPU æ”¯æŒ (NVIDIA)
pip install ninetoothed ntops torch triton
```

### åŸºç¡€ç”¨æ³•

```python
import torch
from infinicore.fusion import FusionScheduler, FusionConfig
from infinicore.fusion.patterns.llm_patterns import create_swiglu_pattern

# 1. åˆ›å»ºèåˆè°ƒåº¦å™¨
config = FusionConfig(enable_fusion=True, debug_mode=True)
scheduler = FusionScheduler(config)

# 2. å‡†å¤‡è¾“å…¥å¼ é‡ï¼ˆGPU ä¸Šï¼‰
device = "cuda"
gate = torch.randn(32, 4096, device=device, dtype=torch.float16)
up = torch.randn(32, 4096, device=device, dtype=torch.float16)

# 3. å®šä¹‰èåˆæ¨¡å¼ï¼ˆSwiGLUï¼‰
graph = create_swiglu_pattern()

# 4. æ‰§è¡Œèåˆ
outputs = scheduler.dispatch(graph, {"gate": gate, "up": up})

# 5. è·å–ç»“æœ
result = outputs["output"]
print(f"è¾“å‡ºå½¢çŠ¶: {result.shape}")
```

### ç¦ç”¨èåˆ

```python
config = FusionConfig(enable_fusion=False)
scheduler = FusionScheduler(config)
# â†’ å°†å•ç‹¬æ‰§è¡Œæ ‡å‡†ç®—å­
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. FusionScheduler (`fusion_scheduler.py`)
è¿è¡Œæ—¶è°ƒåº¦å™¨ï¼Œè´Ÿè´£ï¼š
- æ¥æ”¶å­å›¾å’Œè¾“å…¥å¼ é‡
- åŸºäºå¯å‘å¼è§„åˆ™å†³å®šæ˜¯å¦èåˆ
- ç®¡ç†å†…æ ¸ç¼“å­˜
- æ‰§è¡Œèåˆæˆ–å›é€€è·¯å¾„

### 2. SubGraph (`subgraph.py`)
ä¸å¯å˜ã€å¯å“ˆå¸Œçš„æ•°æ®ç»“æ„ï¼š
- `OpNode`: å•ä¸ªç®—å­èŠ‚ç‚¹
- `SubGraph`: å…·æœ‰æ•°æ®ä¾èµ–çš„ç®—å­åºåˆ—
- é€šè¿‡ `__hash__` å’Œ `cache_key()` æ”¯æŒç¼“å­˜

### 3. FusionConfig (`fusion_config.py`)
é…ç½®é€‰é¡¹ï¼š
```python
@dataclass
class FusionConfig:
    enable_fusion: bool = True        # æ€»å¼€å…³
    enable_cache: bool = True         # å†…æ ¸ç¼“å­˜
    min_nodes: int = 2                # æœ€å°èåˆèŠ‚ç‚¹æ•°
    min_tensor_size: int = 1024       # æœ€å°å¼ é‡å…ƒç´ æ•°
    op_whitelist: Set[str] = DEFAULT_WHITELIST
    fallback_on_error: bool = True    # è‡ªåŠ¨å›é€€
    debug_mode: bool = False          # è¯¦ç»†æ—¥å¿—
```

### 4. FusionHeuristics (`heuristics.py`)
èåˆå†³ç­–çš„é™æ€è§„åˆ™ï¼š
- æ£€æŸ¥ç®—å­ç™½åå•
- æ£€æŸ¥å¼ é‡å¤§å°é˜ˆå€¼
- æ£€æŸ¥èŠ‚ç‚¹æ•°é˜ˆå€¼

### 5. KernelCompiler (`kernel_compiler.py`)
å°†å­å›¾ç¼–è¯‘ä¸ºå¯æ‰§è¡Œå†…æ ¸ï¼š
- ä½¿ç”¨ `ninetoothed` DSL ç”Ÿæˆå†…æ ¸
- åˆ©ç”¨ `ntops` å®ç°ç®—å­
- è¿”å›å¯è°ƒç”¨çš„ `CompiledKernel` å¯¹è±¡

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡ŒåŸºå‡†æµ‹è¯•è„šæœ¬ï¼š

```bash
cd InfiniCore
source ../activate_infini_env.sh

# æµ‹è¯• SwiGLU èåˆ (batch_size=32, hidden_dim=4096)
python test/infinicore/bench_fusion.py \
    --batch_size 32 \
    --hidden_dim 4096 \
    --runs 100
```

**é¢„æœŸè¾“å‡º**ï¼š
```
Benchmarking with Batch Size: 32, Hidden Dim: 4096, Device: cuda
[Standard (Fallback)] Avg Latency: 0.4500 ms
[Fused (Triton)] Avg Latency: 0.1800 ms
Speedup: 60.00%
```

### è‡ªå®šä¹‰åŸºå‡†æµ‹è¯•

```python
from infinicore.fusion import FusionScheduler
import time

def benchmark_fusion():
    # è®¾ç½®
    config = FusionConfig(enable_fusion=True)
    scheduler = FusionScheduler(config)
    graph = create_swiglu_pattern()

    inputs = {
        "gate": torch.randn(32, 4096, device="cuda", dtype=torch.float16),
        "up": torch.randn(32, 4096, device="cuda", dtype=torch.float16),
    }

    # é¢„çƒ­
    for _ in range(10):
        scheduler.dispatch(graph, inputs)

    # åŸºå‡†æµ‹è¯•
    torch.cuda.synchronize()
    start = time.perf_counter()
    for _ in range(100):
        scheduler.dispatch(graph, inputs)
    torch.cuda.synchronize()
    end = time.perf_counter()

    avg_latency_ms = (end - start) / 100 * 1000
    print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency_ms:.4f} ms")
```

---

## ğŸ§ª æµ‹è¯•

### å•å…ƒæµ‹è¯•

```bash
cd InfiniCore
python -m pytest test/infinicore/test_fusion_scheduler.py -v
```

**æµ‹è¯•è¦†ç›–**ï¼š
- âœ… SubGraph å“ˆå¸Œå’Œç¼“å­˜é”®ç”Ÿæˆ
- âœ… FusionConfig å‚æ•°éªŒè¯
- âœ… å¯å‘å¼å†³ç­–è§„åˆ™
- âœ… è°ƒåº¦å™¨åˆ†å‘é€»è¾‘
- âœ… LLM æ¨¡å¼å®šä¹‰

**é¢„æœŸç»“æœ**ï¼š
```
======================== 18 passed in 1.02s ========================
```

### é›†æˆæµ‹è¯•

```bash
# æµ‹è¯•ä¸ ntops çš„èåˆé›†æˆ
python -m pytest test/infinicore/test_fusion_integration.py -v

# æµ‹è¯•æ•°å€¼å‡†ç¡®æ€§
python -m pytest test/infinicore/test_fusion_ntops.py -v
```

---

## ğŸ¨ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ç®—å­æ³¨å†Œ

```python
scheduler = FusionScheduler()

# æ³¨å†Œè‡ªå®šä¹‰ç®—å­ç”¨äºå›é€€
def my_custom_op(x, y, scale=1.0):
    return (x + y) * scale

scheduler.register_op("custom_add_scale", my_custom_op)

# åœ¨ SubGraph ä¸­ä½¿ç”¨
graph = SubGraph(
    nodes=(OpNode("custom_add_scale", inputs=("x", "y"), outputs=("out",)),),
    input_names=("x", "y"),
    output_names=("out"),
)
```

### ç¼“å­˜ç®¡ç†

```python
scheduler = FusionScheduler()

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = scheduler.get_cache_stats()
print(f"ç¼“å­˜å¤§å°: {stats['size']}")

# æ¸…ç©ºç¼“å­˜
scheduler.clear_cache()
```

### è°ƒè¯•æ¨¡å¼

```python
config = FusionConfig(debug_mode=True)
scheduler = FusionScheduler(config)

# â†’ æ‰“å°è¯¦ç»†æ—¥å¿—:
# [FusionScheduler] Cache hit: a3f2c8b1d4e5f6a7
# [FusionScheduler] Compilation success: a3f2c8b1d4e5f6a7
# [FusionScheduler] Fallback execution for graph with 2 nodes
```

---

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„

```
InfiniCore/
â”œâ”€â”€ python/infinicore/fusion/
â”‚   â”œâ”€â”€ fusion_scheduler.py    # è¿è¡Œæ—¶è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ fusion_config.py       # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ heuristics.py          # èåˆå†³ç­–è§„åˆ™
â”‚   â”œâ”€â”€ kernel_compiler.py     # å­å›¾ â†’ Triton å†…æ ¸
â”‚   â”œâ”€â”€ subgraph.py            # æ•°æ®ç»“æ„ (OpNode, SubGraph)
â”‚   â””â”€â”€ patterns/
â”‚       â””â”€â”€ llm_patterns.py    # é¢„å®šä¹‰ LLM èåˆæ¨¡å¼
â”œâ”€â”€ test/infinicore/
â”‚   â”œâ”€â”€ test_fusion_scheduler.py      # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_fusion_integration.py    # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_fusion_ntops.py          # æ•°å€¼å‡†ç¡®æ€§æµ‹è¯•
â”‚   â””â”€â”€ bench_fusion.py               # æ€§èƒ½åŸºå‡†æµ‹è¯•
â””â”€â”€ README.md
```

### æ·»åŠ æ–°çš„èåˆæ¨¡å¼

1. **åœ¨ `patterns/llm_patterns.py` ä¸­å®šä¹‰æ¨¡å¼**ï¼š
```python
def create_my_pattern() -> SubGraph:
    return SubGraph(...)
```

2. **åœ¨ `fusion_scheduler.py:_init_op_registry()` ä¸­æ³¨å†Œç®—å­**ï¼š
```python
self._op_registry["my_op"] = F.my_op
```

3. **æ›´æ–° `fusion_config.py` ä¸­çš„ç™½åå•**ï¼š
```python
DEFAULT_WHITELIST = {..., "my_op"}
```

4. **åœ¨ `test_fusion_scheduler.py` ä¸­æ·»åŠ æµ‹è¯•**ï¼š
```python
def test_my_pattern():
    pattern = create_my_pattern()
    assert len(pattern) == 2
```

---

## ğŸ“– ç›¸å…³é¡¹ç›®

- **[ninetoothed](../ninetoothed)**: ç¬¦å·åŒ– GPU å†…æ ¸ç¼–è¯‘å™¨ï¼ˆç”Ÿæˆ Triton å†…æ ¸ï¼‰
- **[ntops](../ntops)**: é«˜æ€§èƒ½ç®—å­åº“ï¼ˆ60+ ä¼˜åŒ–ç®—å­ï¼‰
- **[InfiniLM](../InfiniLM)**: ä½¿ç”¨èåˆçš„ LLM æ¨ç†å¼•æ“
- **[InfiniTrain](../InfiniTrain)**: åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. ä¸ºæ–°æ¨¡å¼æ·»åŠ æµ‹è¯•
4. æäº¤ Pull Request

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œé¢„æäº¤æ£€æŸ¥
pre-commit run --all-files

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
pytest test/ -v
```

---

## ğŸ“ è®¸å¯è¯

Apache License 2.0

---

## ğŸ™ è‡´è°¢

- **Triton Language**: OpenAI çš„ GPU ç¼–ç¨‹è¯­è¨€
- **PyTorch**: å¼ é‡è®¡ç®—æ¡†æ¶
- **LLaMA, Mistral**: å¯å‘èåˆæ¨¡å¼çš„ LLM æ¶æ„
